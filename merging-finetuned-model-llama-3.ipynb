{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle'):\n    for filename in filenames:\n        if 'doctor' in filename:\n            print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:08:58.303602Z","iopub.execute_input":"2025-03-24T21:08:58.304021Z","iopub.status.idle":"2025-03-24T21:08:58.322140Z","shell.execute_reply.started":"2025-03-24T21:08:58.303992Z","shell.execute_reply":"2025-03-24T21:08:58.321097Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:39:29.141535Z","iopub.execute_input":"2025-03-24T21:39:29.141736Z","iopub.status.idle":"2025-03-24T21:40:01.429164Z","shell.execute_reply.started":"2025-03-24T21:39:29.141717Z","shell.execute_reply":"2025-03-24T21:40:01.428038Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"hf\")\nlogin(token = hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:40:01.430266Z","iopub.execute_input":"2025-03-24T21:40:01.430582Z","iopub.status.idle":"2025-03-24T21:40:02.590966Z","shell.execute_reply.started":"2025-03-24T21:40:01.430557Z","shell.execute_reply":"2025-03-24T21:40:02.590313Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\nnew_model = \"/kaggle/input/finetuning-llama-3/llama-3-8b-chat-doctor/\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:40:02.591679Z","iopub.execute_input":"2025-03-24T21:40:02.591875Z","iopub.status.idle":"2025-03-24T21:40:02.595326Z","shell.execute_reply.started":"2025-03-24T21:40:02.591859Z","shell.execute_reply":"2025-03-24T21:40:02.594582Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom peft import PeftModel\nimport torch\nfrom trl import setup_chat_format\n# Reload tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\nbase_model_reload = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\n\nbase_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:40:02.595988Z","iopub.execute_input":"2025-03-24T21:40:02.596262Z","iopub.status.idle":"2025-03-24T21:41:29.725631Z","shell.execute_reply.started":"2025-03-24T21:40:02.596235Z","shell.execute_reply":"2025-03-24T21:41:29.724268Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02f8fc1fd7f84f248eaff89eebe6a8ed"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-e6253e893e3e>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mbase_model_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_chat_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/models/utils.py\u001b[0m in \u001b[0;36msetup_chat_format\u001b[0;34m(model, tokenizer, format, resize_to_multiple_of)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# check if model already had a chat template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_template\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0;34m\"Chat template is already added to the tokenizer. If you want to overwrite it, please set it to None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: Chat template is already added to the tokenizer. If you want to overwrite it, please set it to None"],"ename":"ValueError","evalue":"Chat template is already added to the tokenizer. If you want to overwrite it, please set it to None","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# Merge adapter with base model\nmodel = PeftModel.from_pretrained(base_model_reload, new_model)\n\nmodel = model.merge_and_unload()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:42:09.648617Z","iopub.execute_input":"2025-03-24T21:42:09.648927Z","iopub.status.idle":"2025-03-24T21:42:29.433627Z","shell.execute_reply.started":"2025-03-24T21:42:09.648887Z","shell.execute_reply":"2025-03-24T21:42:29.432916Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\": \"Hello doctor, I have bad acne. How do I get rid of it?\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\noutputs = pipe(prompt, max_new_tokens=120, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:49:39.799561Z","iopub.execute_input":"2025-03-24T21:49:39.799804Z","iopub.status.idle":"2025-03-24T21:56:49.013180Z","shell.execute_reply.started":"2025-03-24T21:49:39.799784Z","shell.execute_reply":"2025-03-24T21:56:49.012364Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nHello doctor, I have bad acne. How do I get rid of it?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nHello,For mild acne, I suggest you apply a mixture of turmeric powder and ginger paste. For more severe acne, consult a dermatologist and get a proper treatment. Hope I have answered your query. Let me know if I can assist you further. Regards, Dr.assistant\n\nHi! Welcome to Healthcaremagic! I can understand your concern. I would suggest you the following treatment options: 1. You can apply a topical retinoid, such as adapalene gel, to the affected areas. 2. You can use a topical antibiotic, such as cl\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model.save_pretrained(\"llama-3-8b-chat-doctor\")\ntokenizer.save_pretrained(\"llama-3-8b-chat-doctor\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:56:49.014046Z","iopub.execute_input":"2025-03-24T21:56:49.014338Z","iopub.status.idle":"2025-03-24T21:57:51.909100Z","shell.execute_reply.started":"2025-03-24T21:56:49.014301Z","shell.execute_reply":"2025-03-24T21:57:51.908347Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3405: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a225b69bb8694305b1c95f16add187ab"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"('llama-3-8b-chat-doctor/tokenizer_config.json',\n 'llama-3-8b-chat-doctor/special_tokens_map.json',\n 'llama-3-8b-chat-doctor/tokenizer.json')"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"model.push_to_hub(\"llama-3-8b-chat-doctor\", use_temp_dir=False)\ntokenizer.push_to_hub(\"llama-3-8b-chat-doctor\", use_temp_dir=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:57:51.909924Z","iopub.execute_input":"2025-03-24T21:57:51.910237Z","iopub.status.idle":"2025-03-24T22:00:34.040559Z","shell.execute_reply.started":"2025-03-24T21:57:51.910209Z","shell.execute_reply":"2025-03-24T22:00:34.039618Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"869c48d8b32c4f39bc1c0affc19bfc6a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3405: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"162e4d9c7165421b9b362a1ec156b0ab"}},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\nNo files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Anandharaju/llama-3-8b-chat-doctor/commit/4a2a2974db164515e6fd1ee259072503e5ae2330', commit_message='Upload tokenizer', commit_description='', oid='4a2a2974db164515e6fd1ee259072503e5ae2330', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Anandharaju/llama-3-8b-chat-doctor', endpoint='https://huggingface.co', repo_type='model', repo_id='Anandharaju/llama-3-8b-chat-doctor'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"!git clone https://github.com/ggerganov/llama.cpp\n%cd llama.cpp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:01:05.218414Z","iopub.execute_input":"2025-03-24T22:01:05.218782Z","iopub.status.idle":"2025-03-24T22:01:17.533882Z","shell.execute_reply.started":"2025-03-24T22:01:05.218755Z","shell.execute_reply":"2025-03-24T22:01:17.533033Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'llama.cpp'...\nremote: Enumerating objects: 47024, done.\u001b[K\nremote: Counting objects: 100% (28/28), done.\u001b[K\nremote: Compressing objects: 100% (17/17), done.\u001b[K\nremote: Total 47024 (delta 19), reused 11 (delta 11), pack-reused 46996 (from 3)\u001b[K\nReceiving objects: 100% (47024/47024), 98.91 MiB | 18.66 MiB/s, done.\nResolving deltas: 100% (33767/33767), done.\n/kaggle/working/llama.cpp\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:01:38.095128Z","iopub.execute_input":"2025-03-24T22:01:38.095438Z","iopub.status.idle":"2025-03-24T22:01:38.654225Z","shell.execute_reply.started":"2025-03-24T22:01:38.095414Z","shell.execute_reply":"2025-03-24T22:01:38.653141Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/llama.cpp\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!ls -lh /kaggle/working/llama-3-8b-chat-doctor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:08:32.059485Z","iopub.execute_input":"2025-03-24T22:08:32.059848Z","iopub.status.idle":"2025-03-24T22:08:32.600174Z","shell.execute_reply.started":"2025-03-24T22:08:32.059818Z","shell.execute_reply":"2025-03-24T22:08:32.599203Z"}},"outputs":[{"name":"stdout","text":"total 15G\n-rw-r--r-- 1 root root  688 Mar 24 21:57 config.json\n-rw-r--r-- 1 root root  121 Mar 24 21:57 generation_config.json\n-rw-r--r-- 1 root root 4.7G Mar 24 21:58 model-00001-of-00004.safetensors\n-rw-r--r-- 1 root root 4.7G Mar 24 21:58 model-00002-of-00004.safetensors\n-rw-r--r-- 1 root root 4.6G Mar 24 21:59 model-00003-of-00004.safetensors\n-rw-r--r-- 1 root root 1.1G Mar 24 21:59 model-00004-of-00004.safetensors\n-rw-r--r-- 1 root root  24K Mar 24 21:59 model.safetensors.index.json\n-rw-r--r-- 1 root root 5.1K Mar 24 22:00 README.md\n-rw-r--r-- 1 root root  301 Mar 24 22:00 special_tokens_map.json\n-rw-r--r-- 1 root root  50K Mar 24 22:00 tokenizer_config.json\n-rw-r--r-- 1 root root  17M Mar 24 22:00 tokenizer.json\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!ls -lh /kaggle/working/llama.cpp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:10:45.777483Z","iopub.execute_input":"2025-03-24T22:10:45.777807Z","iopub.status.idle":"2025-03-24T22:10:46.321228Z","shell.execute_reply.started":"2025-03-24T22:10:45.777781Z","shell.execute_reply":"2025-03-24T22:10:46.320239Z"}},"outputs":[{"name":"stdout","text":"total 716K\n-rw-r--r--  1 root root  47K Mar 24 22:01 AUTHORS\n-rwxr-xr-x  1 root root  21K Mar 24 22:01 build-xcframework.sh\ndrwxr-xr-x  2 root root 4.0K Mar 24 22:01 ci\ndrwxr-xr-x  2 root root 4.0K Mar 24 22:01 cmake\n-rw-r--r--  1 root root 7.3K Mar 24 22:01 CMakeLists.txt\n-rw-r--r--  1 root root 4.6K Mar 24 22:01 CMakePresets.json\n-rw-r--r--  1 root root  437 Mar 24 22:01 CODEOWNERS\ndrwxr-xr-x  4 root root 4.0K Mar 24 22:01 common\n-rw-r--r--  1 root root 6.4K Mar 24 22:01 CONTRIBUTING.md\n-rwxr-xr-x  1 root root 243K Mar 24 22:01 convert_hf_to_gguf.py\n-rwxr-xr-x  1 root root  18K Mar 24 22:01 convert_hf_to_gguf_update.py\n-rwxr-xr-x  1 root root  19K Mar 24 22:01 convert_llama_ggml_to_gguf.py\n-rwxr-xr-x  1 root root  19K Mar 24 22:01 convert_lora_to_gguf.py\ndrwxr-xr-x  4 root root 4.0K Mar 24 22:01 docs\ndrwxr-xr-x 45 root root 4.0K Mar 24 22:01 examples\n-rw-r--r--  1 root root 1.6K Mar 24 22:01 flake.lock\n-rw-r--r--  1 root root 7.3K Mar 24 22:01 flake.nix\ndrwxr-xr-x  5 root root 4.0K Mar 24 22:01 ggml\ndrwxr-xr-x  5 root root 4.0K Mar 24 22:01 gguf-py\ndrwxr-xr-x  2 root root 4.0K Mar 24 22:01 grammars\ndrwxr-xr-x  2 root root 4.0K Mar 24 22:01 include\n-rw-r--r--  1 root root 1.1K Mar 24 22:01 LICENSE\n-rw-r--r--  1 root root  50K Mar 24 22:01 Makefile\ndrwxr-xr-x  2 root root 4.0K Mar 24 22:01 media\ndrwxr-xr-x  3 root root 4.0K Mar 24 22:01 models\n-rw-r--r--  1 root root  163 Mar 24 22:01 mypy.ini\ndrwxr-xr-x  3 root root 4.0K Mar 24 22:01 pocs\n-rw-r--r--  1 root root 122K Mar 24 22:01 poetry.lock\ndrwxr-xr-x  2 root root 4.0K Mar 24 22:01 prompts\n-rw-r--r--  1 root root 1.3K Mar 24 22:01 pyproject.toml\n-rw-r--r--  1 root root  619 Mar 24 22:01 pyrightconfig.json\n-rw-r--r--  1 root root  27K Mar 24 22:01 README.md\ndrwxr-xr-x  2 root root 4.0K Mar 24 22:01 requirements\n-rw-r--r--  1 root root  551 Mar 24 22:01 requirements.txt\ndrwxr-xr-x  3 root root 4.0K Mar 24 22:01 scripts\n-rw-r--r--  1 root root 5.0K Mar 24 22:01 SECURITY.md\ndrwxr-xr-x  2 root root 4.0K Mar 24 22:01 src\ndrwxr-xr-x  2 root root 4.0K Mar 24 22:01 tests\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"%pip install -r requirements.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/llama.cpp\n!python convert_hf_to_gguf.py /kaggle/input/llama-3-8b-chat-doctor --outfile /kaggle/working/ai_doctor.gguf --outtype f16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:14:13.293575Z","iopub.execute_input":"2025-03-24T22:14:13.293950Z","iopub.status.idle":"2025-03-24T22:14:15.655099Z","shell.execute_reply.started":"2025-03-24T22:14:13.293921Z","shell.execute_reply":"2025-03-24T22:14:15.653982Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/llama.cpp\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"!ls -lh /kaggle/input/finetuning-llama-3/llama-3-8b-chat-doctor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:32:07.515768Z","iopub.execute_input":"2025-03-24T21:32:07.516140Z","iopub.status.idle":"2025-03-24T21:32:08.071279Z","shell.execute_reply.started":"2025-03-24T21:32:07.516112Z","shell.execute_reply":"2025-03-24T21:32:08.070092Z"}},"outputs":[{"name":"stdout","text":"total 161M\n-rw-r--r-- 1 nobody nogroup  880 Mar 24 18:49 adapter_config.json\n-rw-r--r-- 1 nobody nogroup 161M Mar 24 18:49 adapter_model.safetensors\ndrwxr-xr-x 2 nobody nogroup    0 Mar 24 18:49 checkpoint-450\n-rw-r--r-- 1 nobody nogroup 5.1K Mar 24 18:49 README.md\n","output_type":"stream"}],"execution_count":102},{"cell_type":"code","source":"%cd /kaggle/input/finetuning-llama-3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:45:13.211334Z","iopub.execute_input":"2025-03-24T23:45:13.211674Z","iopub.status.idle":"2025-03-24T23:45:13.220216Z","shell.execute_reply.started":"2025-03-24T23:45:13.211643Z","shell.execute_reply":"2025-03-24T23:45:13.219361Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/finetuning-llama-3\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!ls -lh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:39:29.296758Z","iopub.execute_input":"2025-03-24T23:39:29.297131Z","iopub.status.idle":"2025-03-24T23:39:29.421114Z","shell.execute_reply.started":"2025-03-24T23:39:29.297100Z","shell.execute_reply":"2025-03-24T23:39:29.420098Z"}},"outputs":[{"name":"stdout","text":"total 352K\n-rw-r--r-- 1 nobody nogroup    0 Mar 24 23:36 custom.css\ndrwxr-xr-x 3 nobody nogroup    0 Mar 24 23:36 llama-3-8b-chat-doctor\n-rw-r--r-- 1 nobody nogroup  31K Mar 24 23:36 __notebook__.ipynb\n-rw-r--r-- 1 nobody nogroup  651 Mar 24 23:36 __output__.json\n-rw-r--r-- 1 nobody nogroup 313K Mar 24 23:36 __results__.html\ndrwxr-xr-x 3 nobody nogroup    0 Mar 24 23:36 wandb\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!zip -r /kaggle/working/doctor.zip llama-3-8b-chat-doctor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:45:17.187575Z","iopub.execute_input":"2025-03-24T23:45:17.187874Z","iopub.status.idle":"2025-03-24T23:45:53.637880Z","shell.execute_reply.started":"2025-03-24T23:45:17.187851Z","shell.execute_reply":"2025-03-24T23:45:53.636779Z"}},"outputs":[{"name":"stdout","text":"  adding: llama-3-8b-chat-doctor/ (stored 0%)\n  adding: llama-3-8b-chat-doctor/adapter_model.safetensors (deflated 7%)\n  adding: llama-3-8b-chat-doctor/adapter_config.json (deflated 56%)\n  adding: llama-3-8b-chat-doctor/README.md (deflated 66%)\n  adding: llama-3-8b-chat-doctor/checkpoint-450/ (stored 0%)\n  adding: llama-3-8b-chat-doctor/checkpoint-450/adapter_model.safetensors (deflated 7%)\n  adding: llama-3-8b-chat-doctor/checkpoint-450/trainer_state.json (deflated 83%)\n  adding: llama-3-8b-chat-doctor/checkpoint-450/training_args.bin (deflated 51%)\n  adding: llama-3-8b-chat-doctor/checkpoint-450/adapter_config.json (deflated 56%)\n  adding: llama-3-8b-chat-doctor/checkpoint-450/README.md (deflated 65%)\n  adding: llama-3-8b-chat-doctor/checkpoint-450/tokenizer.json (deflated 85%)\n  adding: llama-3-8b-chat-doctor/checkpoint-450/tokenizer_config.json (deflated 96%)\n  adding: llama-3-8b-chat-doctor/checkpoint-450/scheduler.pt (deflated 56%)\n  adding: llama-3-8b-chat-doctor/checkpoint-450/special_tokens_map.json (deflated 64%)\n  adding: llama-3-8b-chat-doctor/checkpoint-450/optimizer.pt (deflated 9%)\n  adding: llama-3-8b-chat-doctor/checkpoint-450/rng_state.pth (deflated 25%)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!ls -lh /kaggle/working/doctor.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:46:41.443725Z","iopub.execute_input":"2025-03-24T23:46:41.444059Z","iopub.status.idle":"2025-03-24T23:46:41.562784Z","shell.execute_reply.started":"2025-03-24T23:46:41.444030Z","shell.execute_reply":"2025-03-24T23:46:41.561666Z"}},"outputs":[{"name":"stdout","text":"-rw-r--r-- 1 root root 591M Mar 24 23:45 /kaggle/working/doctor.zip\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'/kaggle/working/doctor.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:49:15.530112Z","iopub.execute_input":"2025-03-24T23:49:15.530518Z","iopub.status.idle":"2025-03-24T23:49:15.536888Z","shell.execute_reply.started":"2025-03-24T23:49:15.530485Z","shell.execute_reply":"2025-03-24T23:49:15.536169Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/doctor.zip","text/html":"<a href='/kaggle/working/doctor.zip' target='_blank'>/kaggle/working/doctor.zip</a><br>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}